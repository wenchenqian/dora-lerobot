**TP-SP**   **在 ****张量并行****（TP） 基础上，****将序列维度****（sequence dim）；每个 GPU 只持有序列的一部分，****共同完成同一个 attention head 的计算****。**

**CP-Ulysses**  **将 ****整个序列切分成多段****，****每个 GPU 独立处理一段****，并通过 ****环状通信****（ring exchange） 获取全局注意力所需的上下文；****不依赖 TP****，可与 DP/TP 正交组合。**

SP  

* **假设****：已在 TP 组内（如 TP=4）**
* **数据分布****：**
  * **输入 **`x: [B, S, H]` → 切分为 `[B, S/TP, H]`，每个 TP rank 持有一段
  * **所有 TP rank 共同计算同一个 token 的 attention****（通过 **`all-gather` 拼回完整序列）
* **典型操作****：**
  * `all-gather` 拼接 Q/K/V（用于 attention）
  * `reduce-scatter` 拆分 output（用于 MLP）

核心区别：**attention 是怎么算的****？**

▶ TP-Sequence Parallel（SP）

* **所有 TP rank 必须协作计算同一个 token 的 attention****。**
* **举例：计算第 100 个 token 的 attention：**
  * **需要知道 ****全部 0\~99 个 token 的 K/V****；**
  * **所以必须通过 **`all-gather` 把所有 rank 的 K/V 拼起来；
  * **通信发生在 attention 之前****。**
* **结果****：每个 rank 输出的是 ****完整 hidden state****（只是 MLP 层做了 reduce-scatter）。**

> **✅ ****本质****：TP 的扩展，****通信是为了“拼回完整上下文”****。**

▶ CP-Ulysses

* **每个 CP rank 独立计算自己那段 token 的 attention****。**
* **举例：rank 0 负责 token 0\~499，rank 1 负责 500\~999；**
  * **rank 0 计算 token 0\~499 时，****通过 all-to-all 从 rank 1 拿到 500\~999 的 K/V****；**
  * **然后拼成完整上下文，算自己的 attention；**
  * **通信是为了“获取其他段的 K/V”****。**
* **结果****：每个 rank 输出的是 ****自己负责那段 token 的 hidden state****。**

> **✅ ****本质****：上下文并行，****通信是为了“跨段获取上下文”****。**

rope绝对位置编码，q vector乘旋转向量，对于高维的就维度划分分组

loss scale 行业大模型增量预训练通常伴随着大量数据以及较长的训练时长，增量预训练完成后通常还需要SFT重新对齐为可对话的chat模型，此时进行评测后发现模型效果劣化严重可能已经浪费了许多宝贵的时间以及算力。megatron原生的loss打印机制为打印一个batch所有token的平均loss
可以反映数据的难易程度，如果某些channel的初始loss较高，且一直不下降，可能是这部分数据比较脏或者比较难，需要进一步检查数据质量
使用BlendedDataset，getitem方法会同时把每条数据的dataset_id返回，之后只需要在getbatch函数中获取进行返回就行，forward_step函数中将datasetid以partial装饰器方式传入loss_func，在loss_func中基于loss,lossmask和datasetid来计算channelloss
1、单独计算每条数据的loss，需区分cp并行场景，先对每个cp的loss和loss_mask进行求和，然后用all_reduce算子在cp组内对loss和loss_mask进行求和 2、使用allgather通信算子聚合不同DP的loss和datasetid 3、使用bincount函数对每个数据集loss求和，并统计每个数据集的条数 4、最后字典返回每个channel中loss和以及条数，如果直接计算每个channel的平均loss，在之后梯度累加过程，channelloss的计算会有小幅偏差
梯度累加主要就是等效实现更大batchsize的梯度更新效果，原生默认每步loss字典内容完全一致，实际存在dp并行数较大，指定数据集数量较多，不同数据配比不均衡等情况，不同loss之间的keys往往不一样
假设有两个 micro-batch： Step 1：channel 0 有 10 条，total_loss=20 → avg=2.0 Step 2：channel 0 有 1 条，total_loss=5 → avg=5.0
若按“平均再平均”： (2.0 + 5.0) / 2 = 3.5  但真实全局平均应为： (20 + 5) / (10 + 1) = 25 / 11 ≈ 2.27


调整不同领域的训练数据配比
参考Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining 调整不同领域的训练数据配比
将sample-level改成domain-level
将channel loss 线性归一化 + 平移变换 缩放到 【-1， 1】,得到 h_i  直接使用原始 loss 会导致： 大 loss 的 channel 主导梯度更新，小 loss 的 channel 被忽略
通过一个线性上界策略得到si，然后softmax 得到每个领域的权重,根据权重对 channel loss 进行加权，对加权的loss反向    batchsize过小，一个batch可能没法采样到所有的领域
global batchsize越大，每个领域包含的样本越多，得到的loss权重越准确，如果 global batchsize很小，loss权重可能会被一些难样本带偏 
模型会自动聚焦于当前更“困难”或更“重要”的领域，实现动态平衡训练
动态调整各领域权重，避免模型偏向“简单”领域    给正样本更高权重，避免模型只学“负样本”

预训练退火阶段混入少量问答对数据可以使模型更好地适应下游任务
单独验证增量预训练过程的正确性：
1）、观察训练过程的 loss 指标 和 grad norm 指标，分析模型训练过程中是否有异常以及模型的收敛情况。
2）、抽取知识密度较高的数据作为测试用例，计算 PPL 指标（增量预训练后的模型应在行业文本上 PPL 有显著降低）
3）、基于文本续写任务：增量预训练后的模型是否能续写出行业相关文本，计算 BLEU 指标（机器翻译常用指标，用于评估模型输出翻译与参考翻译的重合度） 和 ROUGE (文本摘要常用指标，用于评估重合度）。

答非所问、胡言乱语  学习不充分：通过训练loss曲线，排查是否欠拟合(学习不充分)。初始loss和终止loss之间是否相差5倍以上。调整不合适的参数  学习率是否在10-5次方~10-6次方范围内； 学习率衰减率是否在0.1~0.01内；
热身比例是否在0.1~0.01内  迭代次数增加，观察loss是否正常下降   增加训练数据量(至少1K以上，推荐单场景5K~10K)

模型答案出现逐字级别重复  推理参数调整   1）随机性参数：调大temperature   2）惩罚性参数：11b、100b模型可以调大presence_penalty参数；38b、71b暂时不支持参数调整。

LoRA 的核心只是把参数更新限制在一个低秩子空间上，但损失函数依然是普通的 cross-entropy，所以在小数据 / 大步长的设置下，依旧可能严重偏离原始分布；同时，rank 也很难选：太小学不动复杂模式，太大又增加参数与显存开销、训练不稳、更容易过拟合。
LoraParallelLinearMoE 适配张量并行的线性层 注入 LoRA 配置 + 替换 tp_layer.LoraParallelLinear
若是 GroupGemmExperts LoraGroupGemmExperts 适配 MoE 专家层

![img.png](img.png)![img_1.png](img_1.png)
Muon 优化器通过矩阵分解，优化了梯度更新方向  过高的Logit数值可能会带来梯度尖刺，严重时会影响训练收敛  MuonClip优化器，在原Muon优化器基础上增加了QK-clip特性，明显降低了损失尖刺出现频率。
RMS系数:梯度平方的指数移动平均（EMA）的平方根，本质上是一个自适应的、基于历史梯度尺度的归一化因子   优化器中，通常使用指数移动平均（EMA）代替简单平均，以更关注近期梯度![img_2.png](img_2.png) ![img_4.png](img_4.png) 作用是![img_5.png](img_5.png) ADAM![img_3.png](img_3.png)

MOE的辅助loss：1、路由概率熵损失：让router输出更均匀的专家分配概率![img_6.png](img_6.png) 2、负载均衡损失 核心思想：
同时考虑 router 的输出概率 和 实际被选中的专家（top-k）。
惩罚“高概率但低使用率”的专家。 ![img_7.png](img_7.png)

MOE的EP切分：EP 和 TP 是 正交的，即 TP 切分是在 每个专家内部 进行的。All-to-All 通信（核心！） 目的：将 tokens 重分布到持有对应 expert 的 GPU 上
操作： 收集所有 DP+TP 组内的 tokens（实际是 token 表示向量）  根据 expert ID，通过 All-to-All 将 tokens 发送给 EP 组中对应的 GPU!  通信开销分析 [img_8.png](img_8.png)

正常增训过程中，可以使用alodra构建instruct, 并通过BON采样，偏好引导等方式获取高质量的答案