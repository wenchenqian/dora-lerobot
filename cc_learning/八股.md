**TP-SP**   **在 ****张量并行****（TP） 基础上，****将序列维度****（sequence dim）；每个 GPU 只持有序列的一部分，****共同完成同一个 attention head 的计算****。**

**CP-Ulysses**  **将 ****整个序列切分成多段****，****每个 GPU 独立处理一段****，并通过 ****环状通信****（ring exchange） 获取全局注意力所需的上下文；****不依赖 TP****，可与 DP/TP 正交组合。**

SP  

* **假设****：已在 TP 组内（如 TP=4）**
* **数据分布****：**
  * **输入 **`x: [B, S, H]` → 切分为 `[B, S/TP, H]`，每个 TP rank 持有一段
  * **所有 TP rank 共同计算同一个 token 的 attention****（通过 **`all-gather` 拼回完整序列）
* **典型操作****：**
  * `all-gather` 拼接 Q/K/V（用于 attention）
  * `reduce-scatter` 拆分 output（用于 MLP）

核心区别：**attention 是怎么算的****？**

▶ TP-Sequence Parallel（SP）

* **所有 TP rank 必须协作计算同一个 token 的 attention****。**
* **举例：计算第 100 个 token 的 attention：**
  * **需要知道 ****全部 0\~99 个 token 的 K/V****；**
  * **所以必须通过 **`all-gather` 把所有 rank 的 K/V 拼起来；
  * **通信发生在 attention 之前****。**
* **结果****：每个 rank 输出的是 ****完整 hidden state****（只是 MLP 层做了 reduce-scatter）。**

> **✅ ****本质****：TP 的扩展，****通信是为了“拼回完整上下文”****。**

▶ CP-Ulysses

* **每个 CP rank 独立计算自己那段 token 的 attention****。**
* **举例：rank 0 负责 token 0\~499，rank 1 负责 500\~999；**
  * **rank 0 计算 token 0\~499 时，****通过 all-to-all 从 rank 1 拿到 500\~999 的 K/V****；**
  * **然后拼成完整上下文，算自己的 attention；**
  * **通信是为了“获取其他段的 K/V”****。**
* **结果****：每个 rank 输出的是 ****自己负责那段 token 的 hidden state****。**

> **✅ ****本质****：上下文并行，****通信是为了“跨段获取上下文”****。**

rope绝对位置编码，q vector乘旋转向量，对于高维的就维度划分分组


