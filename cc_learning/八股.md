**TP-SP**   **在 ****张量并行****（TP） 基础上，****将序列维度****（sequence dim）；每个 GPU 只持有序列的一部分，****共同完成同一个 attention head 的计算****。**

**CP-Ulysses**  **将 ****整个序列切分成多段****，****每个 GPU 独立处理一段****，并通过 ****环状通信****（ring exchange） 获取全局注意力所需的上下文；****不依赖 TP****，可与 DP/TP 正交组合。**

SP  

* **假设****：已在 TP 组内（如 TP=4）**
* **数据分布****：**
  * **输入 **`x: [B, S, H]` → 切分为 `[B, S/TP, H]`，每个 TP rank 持有一段
  * **所有 TP rank 共同计算同一个 token 的 attention****（通过 **`all-gather` 拼回完整序列）
* **典型操作****：**
  * `all-gather` 拼接 Q/K/V（用于 attention）
  * `reduce-scatter` 拆分 output（用于 MLP）

核心区别：**attention 是怎么算的****？**

▶ TP-Sequence Parallel（SP）

* **所有 TP rank 必须协作计算同一个 token 的 attention****。**
* **举例：计算第 100 个 token 的 attention：**
  * **需要知道 ****全部 0\~99 个 token 的 K/V****；**
  * **所以必须通过 **`all-gather` 把所有 rank 的 K/V 拼起来；
  * **通信发生在 attention 之前****。**
* **结果****：每个 rank 输出的是 ****完整 hidden state****（只是 MLP 层做了 reduce-scatter）。**

> **✅ ****本质****：TP 的扩展，****通信是为了“拼回完整上下文”****。**

▶ CP-Ulysses

* **每个 CP rank 独立计算自己那段 token 的 attention****。**
* **举例：rank 0 负责 token 0\~499，rank 1 负责 500\~999；**
  * **rank 0 计算 token 0\~499 时，****通过 all-to-all 从 rank 1 拿到 500\~999 的 K/V****；**
  * **然后拼成完整上下文，算自己的 attention；**
  * **通信是为了“获取其他段的 K/V”****。**
* **结果****：每个 rank 输出的是 ****自己负责那段 token 的 hidden state****。**

> **✅ ****本质****：上下文并行，****通信是为了“跨段获取上下文”****。**

rope绝对位置编码，q vector乘旋转向量，对于高维的就维度划分分组

loss scale 行业大模型增量预训练通常伴随着大量数据以及较长的训练时长，增量预训练完成后通常还需要SFT重新对齐为可对话的chat模型，此时进行评测后发现模型效果劣化严重可能已经浪费了许多宝贵的时间以及算力。megatron原生的loss打印机制为打印一个batch所有token的平均loss
可以反映数据的难易程度，如果某些channel的初始loss较高，且一直不下降，可能是这部分数据比较脏或者比较难，需要进一步检查数据质量
使用BlendedDataset，getitem方法会同时把每条数据的dataset_id返回，之后只需要在getbatch函数中获取进行返回就行，forward_step函数中将datasetid以partial装饰器方式传入loss_func，在loss_func中基于loss,lossmask和datasetid来计算channelloss
1、单独计算每条数据的loss，需区分cp并行场景，先对每个cp的loss和loss_mask进行求和，然后用all_reduce算子在cp组内对loss和loss_mask进行求和 2、使用allgather通信算子聚合不同DP的loss和datasetid 3、使用bincount函数对每个数据集loss求和，并统计每个数据集的条数 4、最后字典返回每个channel中loss和以及条数，如果直接计算每个channel的平均loss，在之后梯度累加过程，channelloss的计算会有小幅偏差
梯度累加主要就是等效实现更大batchsize的梯度更新效果，原生默认每步loss字典内容完全一致，实际存在dp并行数较大，指定数据集数量较多，不同数据配比不均衡等情况，不同loss之间的keys往往不一样
假设有两个 micro-batch： Step 1：channel 0 有 10 条，total_loss=20 → avg=2.0 Step 2：channel 0 有 1 条，total_loss=5 → avg=5.0
若按“平均再平均”： (2.0 + 5.0) / 2 = 3.5  但真实全局平均应为： (20 + 5) / (10 + 1) = 25 / 11 ≈ 2.27


调整不同领域的训练数据配比
参考Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining 调整不同领域的训练数据配比
将sample-level改成domain-level
将channel loss 线性归一化 + 平移变换 缩放到 【-1， 1】,得到 h_i  直接使用原始 loss 会导致： 大 loss 的 channel 主导梯度更新，小 loss 的 channel 被忽略
通过一个线性上界策略得到si，然后softmax 得到每个领域的权重,根据权重对 channel loss 进行加权，对加权的loss反向    batchsize过小，一个batch可能没法采样到所有的领域
global batchsize越大，每个领域包含的样本越多，得到的loss权重越准确，如果 global batchsize很小，loss权重可能会被一些难样本带偏 
模型会自动聚焦于当前更“困难”或更“重要”的领域，实现动态平衡训练
动态调整各领域权重，避免模型偏向“简单”领域    给正样本更高权重，避免模型只学“负样本”

预训练退火阶段混入少量问答对数据可以使模型更好地适应下游任务
单独验证增量预训练过程的正确性：
1）、观察训练过程的 loss 指标 和 grad norm 指标，分析模型训练过程中是否有异常以及模型的收敛情况。
2）、抽取知识密度较高的数据作为测试用例，计算 PPL 指标（增量预训练后的模型应在行业文本上 PPL 有显著降低）
3）、基于文本续写任务：增量预训练后的模型是否能续写出行业相关文本，计算 BLEU 指标（机器翻译常用指标，用于评估模型输出翻译与参考翻译的重合度） 和 ROUGE (文本摘要常用指标，用于评估重合度）。

答非所问、胡言乱语  学习不充分：通过训练loss曲线，排查是否欠拟合(学习不充分)。初始loss和终止loss之间是否相差5倍以上。调整不合适的参数  学习率是否在10-5次方~10-6次方范围内； 学习率衰减率是否在0.1~0.01内；
热身比例是否在0.1~0.01内  迭代次数增加，观察loss是否正常下降   增加训练数据量(至少1K以上，推荐单场景5K~10K)

模型答案出现逐字级别重复  推理参数调整   1）随机性参数：调大temperature   2）惩罚性参数：11b、100b模型可以调大presence_penalty参数；38b、71b暂时不支持参数调整。

LoRA 的核心只是把参数更新限制在一个低秩子空间上，但损失函数依然是普通的 cross-entropy，所以在小数据 / 大步长的设置下，依旧可能严重偏离原始分布；同时，rank 也很难选：太小学不动复杂模式，太大又增加参数与显存开销、训练不稳、更容易过拟合。
LoraParallelLinearMoE 适配张量并行的线性层 注入 LoRA 配置 + 替换 tp_layer.LoraParallelLinear
若是 GroupGemmExperts LoraGroupGemmExperts 适配 MoE 专家层

RMSnorm:只做均方根归一化，减少计算

DSA主要解决Q只和K个（KV）计算，不用每个都计算。有个lightening——indexer和topk-select

![img.png](img.png)![img_1.png](img_1.png)
Muon 优化器通过矩阵分解，优化了梯度更新方向  过高的Logit数值可能会带来梯度尖刺，严重时会影响训练收敛  MuonClip优化器，在原Muon优化器基础上增加了QK-clip特性，明显降低了损失尖刺出现频率。
RMS系数:梯度平方的指数移动平均（EMA）的平方根，本质上是一个自适应的、基于历史梯度尺度的归一化因子   优化器中，通常使用指数移动平均（EMA）代替简单平均，以更关注近期梯度![img_2.png](img_2.png) ![img_4.png](img_4.png) 作用是![img_5.png](img_5.png) ADAM![img_3.png](img_3.png)

计算和通信重叠，在alltoall的时候，分成两个minibatch，前向dispatch的时候后向计算2、在一个batch的时候，后向传梯度和计算W偏导并行
共享专家：缓解专家坍塌，提升通用能力，增强泛化性，qwen3没有共享专家
MOE的辅助loss：1、路由概率熵损失：让router输出更均匀的专家分配概率![img_6.png](img_6.png) 2、负载均衡损失 核心思想：
同时考虑 router 的输出概率 和 实际被选中的专家（top-k）。
惩罚“高概率但低使用率”的专家。解决单条样本的负载不均衡问题 ![img_7.png](img_7.png)  加入偏执项b

MOE的EP切分：EP 和 TP 是 正交的，即 TP 切分是在 每个专家内部 进行的。All-to-All 通信（核心！） 目的：将 tokens 重分布到持有对应 expert 的 GPU 上
操作： 收集所有 DP+TP 组内的 tokens（实际是 token 表示向量）  根据 expert ID，通过 All-to-All 将 tokens 发送给 EP 组中对应的 GPU!  通信开销分析 [img_8.png](img_8.png)

MTP预测：训练看的远，加入MTP损失，推理的话就是投机算法

正常增训过程中，可以使用alodra构建instruct, 并通过BON采样，偏好引导等方式获取高质量的答案

一个175B的GPT-3模型需要（175B * 4bytes）就是700GB模型参数空间，从而梯度也是700G，优化器状态是1400G，一共2.8TB

开源金融数据Fin-Eval.领域数据：优先考虑涉及计算等的复杂场景数据，如金融领域的精算、财报分析、投资组合优化等场景。
领域和通用任务配比： 金融领域SFT数据量级比较大，当前领域和通用任务整体维持在1:1的比例，可以在保证通用能力不下降的情况下，整体提升领域任务效果。
不同任务类型的数据配比： self-qa, 网页挖掘问答对，相似检索/标签检索等方式获取的数据， 通常量级非常大，且没有具体的任务指向性（指令明确告诉模型 角色 + 目标 + 约束）。缺乏真实用户意图；无场景约束。实际训练配比过程中，一般可以只训练一个epoch。 而针对具体领域任务针对性构造的数据，则可以适当提升训练epoch数量，通常为3个epoch.
根据训练结果适当调整不同任务数据配比：任务类数据在训练过程中，最好在中间过程中将模型保存并进行自动化评测， 监控每个任务的训练动态， 验证模型在某些任务上是否收敛。 如果某个任务在训练过程中一直成增长趋势， 则可以适当增加该任务训练的epoch数量。 如果某个任务在加入了对应的构造数据之后，反而效果在下降，则大概率数据质量有问题。可以考虑通过偏好引导和BON的方式产生更好的答案。

self-qa:将原始文本数据转换为问答格式，支持（如续写、作文、补全等），适用于构建 QA 数据集。1、清洗文本2、根据内容特征选择续写/补全3、规则生成
单轮对话数据合成：1、选择参看资料，文档片段越长，效果越不好，因此建议对先文档进行切片，使用较短的文档进行问答数据合成，2、题型分流：合成的问答数据需要同时包含多种题型3、把文档片段拼入各类题型模板3、虽然Self-QA是基于给定文本生成问答数据，但仍然无法100%避免幻觉（比如答案未使用文档片段的内容，而是自己编纂），因此需要将模型生成的问答数据（尤其是解释和答案）与给定的文档片段计算相似度，将相似度低于阈值的问答对滤除或者重新生成。
注： 文本相似度算法建议采用ROUGE。 阈值设置没有固定值，需要根据实际情况决定，一般在0.2左右。Reference: “宁德时代2023年海外营收占比23.4%” Candidate: “2023年宁德时代海外收入占比为23.4%” → ROUGE 会认为两者高度相似（因 n-gram 重叠高）




数据管理：基于embedding（gte-large-zh）和 FAISS 向量索引的 SFT 数据去重与查重系统：金融 QA 对语义敏感，阈值不宜过低： 0.95~0.98：严格去重（推荐）

self-instruct（提升问题多样性）:单轮对话数据合成流程：1、选择种子指令&确认指令生成数量2、拼入指令生成提示词模板这里提供了两类不同的提示词模板，可根据实际情况选择： 相似指令生成：生成的指令与提供种子指令的意图、话题、问题类型、风格、难度等更加相似。 指令泛化：基于提供的种子指令进行泛化，生成的指令在保证话题相同的基础上会更加多样化。3、将生成的指令x拼入答案生成提示词模板
通过Self-Instruct生成指令（问题）后，再次调用大模型生成对应的答案。 若生成了多条指令，则需要分别对每条指令依次调用大模型获取每个问题的答案。通常情况下，Self-Instruct或Self-QA合成的指令（问题）都较为简单。在模型SFT时，微调数据中建议包含一定比例的复杂问题（复杂题型、长问题、多个子问题等），以提升模型的泛化能力。同时，考虑到现实世界的大部分问题仍然比较简单，因此也涉及将复杂问题改写简单的场景。
本方法适用于基于给定的指令（问题），通过大模型将其改写为难度更高或者更低的指令（问题）。
注： 给定的指令（old_instruction）只能为1条。 生成指令的数量（num_response_instructions）一般大于1，理论上生成的指令越多质量越差。

拼接种子指令和指令生成数量后，相似指令生成提示词如下所示：
你是一个指令构造专家，你擅长根据要求构造指令。
 
基本要求：
- 如果生成多个指令，需要保证指令的具体任务、措辞以及复杂度的多样性
- 生成的指令相互独立(每个指令都不需要依赖上一个指令的信息， 每个指令都需要包含任务的完整信息， 包含完成该指令所需要的全部背景信息)
- 生成的指令应该能够被生成式语言模型解决，内容完整
 
生成与种子指令相似的指令
指令应该尽量与种子指令的长度保持一致。
 
种子指令： 种子1：什么是社会消费品零售总额？ 种子2：请解释国内生产总值的含义及其计算方式。 种子3：财政赤字是如何产生的？
 
请构造5个相似的指令。

指令质量过滤器，支持自定义打分维度，默认的质量评估打分维度如下:"意图明确性": "判断<指令>的意图是否明确，如果明确，则打1分；否则，打0分"2、"语义清晰性": "判断<指令>的语义是否清晰，如果清晰，则打1分；否则，打0分"3、"指令完整性": "判断<指令>给出的信息是否全面，上下文是否完整, 是否可以回答，如果可回答，则打1分；否则，打0分"
指令质量维度 custom_dimensions = {
    "金融实体明确性": "判断<指令>是否明确提及金融实体（如公司名、产品名、指标名），若是，打1分；否则0分",
    "事实可验证性": "判断<指令>是否涉及可查证的事实（如财报数据、政策条款），若是，打1分；否则0分",
    "任务类型明确性": "判断<指令>是否明确任务类型（如归因、对比、解释、预测），若是，打1分；否则0分"
}
### 领域微调分享
在人设隔离方案中，“使用通用meta prompt的通用数据”、“使用通用meta prompt的领域数据”以及“使用领域meta prompt的领域数据”三者需要按照一定的比例混合。1、数据回放（混合训练，配比1:1） 2、人设隔离（System Prompt 强引导） 3、 平行数据（为同一任务构造“通用表述”与“领域表述”的成对样本，引导模型学会领域风格迁移，通用meta prompt的领域数据）
总体方案：
领域持续训练阶段混合高质量通用任务指令数据，通过数据回放（exemplar replay）的持续学习方案，防止模型遗忘通用任务能力
训练阶段通过不同自然语言指令前缀（meta prompt），区分不同领域的指令数据和通用任务数据，确保不同领域数据能得到充分的训练，且不互相干扰
通过构造平行数据的方式，在通用任务的自然语言指令前缀（meta prompt），集成整合各领域能力和通用任务能力，实现模型部署阶段，使用统一meta prompt，处理通用任务和各领域任务，避免针对领域、任务动态选择不同的meta pr
### 增量预训练
10B tokens以上可以尝试增量预训练；10Btokens以下可以将预训练和通用SFT混合训练。