**TP-SP**   **在 ****张量并行****（TP） 基础上，****将序列维度****（sequence dim）；每个 GPU 只持有序列的一部分，****共同完成同一个 attention head 的计算****。**

**CP-Ulysses**  **将 ****整个序列切分成多段****，****每个 GPU 独立处理一段****，并通过 ****环状通信****（ring exchange） 获取全局注意力所需的上下文；****不依赖 TP****，可与 DP/TP 正交组合。**

SP  

* **假设****：已在 TP 组内（如 TP=4）**
* **数据分布****：**
  * **输入 **`x: [B, S, H]` → 切分为 `[B, S/TP, H]`，每个 TP rank 持有一段
  * **所有 TP rank 共同计算同一个 token 的 attention****（通过 **`all-gather` 拼回完整序列）
* **典型操作****：**
  * `all-gather` 拼接 Q/K/V（用于 attention）
  * `reduce-scatter` 拆分 output（用于 MLP）

核心区别：**attention 是怎么算的****？**

▶ TP-Sequence Parallel（SP）

* **所有 TP rank 必须协作计算同一个 token 的 attention****。**
* **举例：计算第 100 个 token 的 attention：**
  * **需要知道 ****全部 0\~99 个 token 的 K/V****；**
  * **所以必须通过 **`all-gather` 把所有 rank 的 K/V 拼起来；
  * **通信发生在 attention 之前****。**
* **结果****：每个 rank 输出的是 ****完整 hidden state****（只是 MLP 层做了 reduce-scatter）。**

> **✅ ****本质****：TP 的扩展，****通信是为了“拼回完整上下文”****。**

▶ CP-Ulysses

* **每个 CP rank 独立计算自己那段 token 的 attention****。**
* **举例：rank 0 负责 token 0\~499，rank 1 负责 500\~999；**
  * **rank 0 计算 token 0\~499 时，****通过 all-to-all 从 rank 1 拿到 500\~999 的 K/V****；**
  * **然后拼成完整上下文，算自己的 attention；**
  * **通信是为了“获取其他段的 K/V”****。**
* **结果****：每个 rank 输出的是 ****自己负责那段 token 的 hidden state****。**

> **✅ ****本质****：上下文并行，****通信是为了“跨段获取上下文”****。**

rope绝对位置编码，q vector乘旋转向量，对于高维的就维度划分分组

关注 Domain-level  data mixing/domain reweighting ，也就是调整不同领域的训练数据配比
参考： Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining 这篇论文的方法
将sample-level改成domain-level
将channel loss 缩放到 【-1， 1】,得到 h_i
softmax 得到每个领域的权重,根据权重对 channel loss 进行加权，对加权的loss反向    batchsize过小，一个batch可能没法采样到所有的领域
模型会自动聚焦于当前更“困难”或更“重要”的领域，实现动态平衡训练
动态调整各领域权重，避免模型偏向“简单”领域    给正样本更高权重，避免模型只学“负样本”

预训练退火阶段混入少量问答对数据可以使模型更好地适应下游任务
单独验证增量预训练过程的正确性：
1）、观察训练过程的 loss 指标 和 grad norm 指标，分析模型训练过程中是否有异常以及模型的收敛情况。
2）、抽取知识密度较高的数据作为测试用例，计算 PPL 指标（增量预训练后的模型应在行业文本上 PPL 有显著降低）
3）、基于文本续写任务：增量预训练后的模型是否能续写出行业相关文本，计算 BLEU 指标（机器翻译常用指标，用于评估模型输出翻译与参考翻译的重合度） 和 ROUGE (文本摘要常用指标，用于评估重合度）。

答非所问、胡言乱语  学习不充分：通过训练loss曲线，排查是否欠拟合(学习不充分)。初始loss和终止loss之间是否相差5倍以上。调整不合适的参数  学习率是否在10-5次方~10-6次方范围内； 学习率衰减率是否在0.1~0.01内；
热身比例是否在0.1~0.01内  迭代次数增加，观察loss是否正常下降   增加训练数据量(至少1K以上，推荐单场景5K~10K)

模型答案出现逐字级别重复  推理参数调整   1）随机性参数：调大temperature   2）惩罚性参数：11b、100b模型可以调大presence_penalty参数；38b、71b暂时不支持参数调整。

LoRA 的核心只是把参数更新限制在一个低秩子空间上，但损失函数依然是普通的 cross-entropy，所以在小数据 / 大步长的设置下，依旧可能严重偏离原始分布；同时，rank 也很难选：太小学不动复杂模式，太大又增加参数与显存开销、训练不稳、更容易过拟合。