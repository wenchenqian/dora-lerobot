**TP-SP**   **在 ****张量并行****（TP） 基础上，****将序列维度****（sequence dim）；每个 GPU 只持有序列的一部分，****共同完成同一个 attention head 的计算****。**

**CP-Ulysses**  **将 ****整个序列切分成多段****，****每个 GPU 独立处理一段****，并通过 ****环状通信****（ring exchange） 获取全局注意力所需的上下文；****不依赖 TP****，可与 DP/TP 正交组合。**

SP* **假设****：已在 TP 组内（如 TP=4）**
* **数据分布****：**
  * **输入 **`x: [B, S, H]` → 切分为 `[B, S/TP, H]`，每个 TP rank 持有一段
  * **所有 TP rank 共同计算同一个 token 的 attention****（通过 **`all-gather` 拼回完整序列）
* **典型操作****：**
  * `all-gather` 拼接 Q/K/V（用于 attention）
  * `reduce-scatter` 拆分 output（用于 MLP）

核心区别：**attention 是怎么算的****？**
▶ TP-Sequence Parallel（SP）
* **所有 TP rank 必须协作计算同一个 token 的 attention****。**
* **举例：计算第 100 个 token 的 attention：**
  * **需要知道 ****全部 0\~99 个 token 的 K/V****；**
  * **所以必须通过 **`all-gather` 把所有 rank 的 K/V 拼起来；
  * **通信发生在 attention 之前****。**
* **结果****：每个 rank 输出的是 ****完整 hidden state****（只是 MLP 层做了 reduce-scatter）。**
> **✅ ****本质****：TP 的扩展，****通信是为了“拼回完整上下文”****。**
▶ CP-Ulysses
* **每个 CP rank 独立计算自己那段 token 的 attention****。**
* **举例：rank 0 负责 token 0\~499，rank 1 负责 500\~999；**
  * **rank 0 计算 token 0\~499 时，****通过 all-to-all 从 rank 1 拿到 500\~999 的 K/V****；**
  * **然后拼成完整上下文，算自己的 attention；**
  * **通信是为了“获取其他段的 K/V”****。**
* **结果****：每个 rank 输出的是 ****自己负责那段 token 的 hidden state****。**
> **✅ ****本质****：上下文并行，****通信是为了“跨段获取上下文”****。**

Token 路由（routing） 需要所有参与 EP 的设备在同一 PP stage 内通信（如 AllToAll）
如果 EP 跨越多个 PP stage，则路由无法完成（因为 activation 还没传到下一个 stage）
高 TP + 高 PP // TP 的 AllReduce + PP 的 P2P 通信竞争带宽
高 EP + 高 TP // GMM 计算 + NPU AllToAll 路由成为瓶颈

rope绝对位置编码，q vector乘旋转向量，对于高维的就维度划分分组。高频：短距离依赖。低频：长距离依赖.rope里每个m对应弧度m*theta—i
解决外推可以用NTK，保留高频信息，高频部分（旋转周期数量足够多）进行外推，低频部分进行内插



loss scale 行业大模型增量预训练通常伴随着大量数据以及较长的训练时长，增量预训练完成后通常还需要SFT重新对齐为可对话的chat模型，此时进行评测后发现模型效果劣化严重可能已经浪费了许多宝贵的时间以及算力。megatron原生的loss打印机制为打印一个batch所有token的平均loss
可以反映数据的难易程度，如果某些channel的初始loss较高，且一直不下降，可能是这部分数据比较脏或者比较难，需要进一步检查数据质量
使用BlendedDataset，getitem方法会同时把每条数据的dataset_id返回，之后只需要在getbatch函数中获取进行返回就行，forward_step函数中将datasetid以partial装饰器方式传入loss_func，在loss_func中基于loss,lossmask和datasetid来计算channelloss
1、单独计算每条数据的loss，需区分cp并行场景，先对每个cp的loss和loss_mask进行求和，然后用all_reduce算子在cp组内对loss和loss_mask进行求和 2、使用allgather通信算子聚合不同DP的loss和datasetid 3、使用bincount函数对每个数据集loss求和，并统计每个数据集的条数 4、最后字典返回每个channel中loss和以及条数，如果直接计算每个channel的平均loss，在之后梯度累加过程，channelloss的计算会有小幅偏差
梯度累加主要就是等效实现更大batchsize的梯度更新效果，原生默认每步loss字典内容完全一致，实际存在dp并行数较大，指定数据集数量较多，不同数据配比不均衡等情况，不同loss之间的keys往往不一样
假设有两个 micro-batch： Step 1：channel 0 有 10 条，total_loss=20 → avg=2.0 Step 2：channel 0 有 1 条，total_loss=5 → avg=5.0
若按“平均再平均”： (2.0 + 5.0) / 2 = 3.5  但真实全局平均应为： (20 + 5) / (10 + 1) = 25 / 11 ≈ 2.27


### 调整不同领域的训练数据配比
参考Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining 调整不同领域的训练数据配比
动机：为什么要在训练过程中动态的reweight？
有些样本训练初期很难学，赋予大权重，随着训练进度学到了，权重可以降低。
样本量很多，无法记录每个样本的损失
将sample-level改成domain-level
将channel loss 线性归一化 + 平移变换 缩放到 【-1， 1】,得到 h_i  直接使用原始 loss 会导致： 大 loss 的 channel 主导梯度更新，小 loss 的 channel 被忽略
通过一个线性上界策略得到si，然后softmax 得到每个领域的权重,根据权重对 channel loss 进行加权，对加权的loss反向    batchsize过小，一个batch可能没法采样到所有的领域
global batchsize越大，每个领域包含的样本越多，得到的loss权重越准确，如果 global batchsize很小，loss权重可能会被一些难样本带偏 
模型会自动聚焦于当前更“困难”或更“重要”的领域，实现动态平衡训练
动态调整各领域权重，避免模型偏向“简单”领域    给正样本更高权重，避免模型只学“负样本”

使用 PipeDream-Flush 调度策略：
一个 global batch 被拆分为多个 micro-batch，分散在不同 pipeline stage 上。
❌ 问题：无法在 global batch 粒度计算总 loss
每个 micro-batch 的 loss 在不同 stage / 不同 GPU 上计算；
没有一个地方能拿到完整的 global batch 所有 loss（因为 PP 是流水线，micro-batch 异步处理）；
→ 无法直接对整个 global batch 做加权（如按领域加权）

算法详细流程：1：Warm-up 阶段（固定权重 = 1）前 N 个 iter（如 1000 steps）：
所有领域 loss 权重 = 1.0；
只记录 loss 到 channel_losses_record，不更新权重；
目的：收集初始 loss 分布，避免冷启动偏差。
2：Warm-up 后，每个 iter 的加权流程
假设当前 global batch 包含来自多个领域的样本（如 2 条金融 + 3 条数学 + 1 条通用）。

a) 对每个领域，计算滑动平均 loss，取最近 10 个 iter 的 loss 均值； 作为该领域“当前难度”的代理指标
b) 应用 LinUpper 策略计算权重：线性归一化 + 上界裁剪 c) 用权重计算当前 micro-batch 的 loss

一、什么是“对 global batch 计算权重”？
这种理想化方案通常指：

当前 global batch 包含：
30 条金融样本 → loss_finance = 1.2
50 条数学样本 → loss_math = 0.8
20 条通用样本 → loss_general = 0.5
聚合整个 global batch 的 per-domain loss；
基于这些 loss 动态计算权重
用这些权重反向传播。
❌ 为什么在 Megatron PP（PipeDream-Flush）下做不到？
global batch 被拆成多个 micro-batch；
每个 micro-batch 在不同 pipeline stage / GPU 上独立计算 loss；
没有一个进程能看到完整的 per-domain loss 分布；
无法在 backward 前完成权重计算（因为 loss 还没全算完）→ 必须等待所有 micro-batch 的 loss 聚合 → 违反 PP 的流水线并行原则 → 性能崩溃。
所以方案是 “用历史代替当前”1、每个 iter，把每个样本的 原始 loss 按 domain 存入 channel_losses_record 2、下一个 iter 开始时，用过去 10 个 iter 的滑动平均 loss 估算各领域难度3、对当前 iter 的每个样本，根据其 domain 查表得权重，加权 loss 4、用当前 iter 的 原始 loss 更新 channel_losses_record


预训练退火阶段混入少量问答对数据可以使模型更好地适应下游任务
单独验证增量预训练过程的正确性：
1）、观察训练过程的 loss 指标 和 grad norm 指标，分析模型训练过程中是否有异常以及模型的收敛情况。
2）、抽取知识密度较高的数据作为测试用例，计算 PPL 指标（增量预训练后的模型应在行业文本上 PPL 有显著降低）
3）、基于文本续写任务：增量预训练后的模型是否能续写出行业相关文本，计算 BLEU 指标（机器翻译常用指标，用于评估模型输出翻译与参考翻译的重合度） 和 ROUGE (文本摘要常用指标，用于评估重合度）。

答非所问、胡言乱语  学习不充分：通过训练loss曲线，排查是否欠拟合(学习不充分)。初始loss和终止loss之间是否相差5倍以上。调整不合适的参数  学习率是否在10-5次方~10-6次方范围内； 学习率衰减率是否在0.1~0.01内；
热身比例是否在0.1~0.01内  迭代次数增加，观察loss是否正常下降   增加训练数据量(至少1K以上，推荐单场景5K~10K)

模型答案出现逐字级别重复  推理参数调整   1）随机性参数：调大temperature   2）惩罚性参数：11b、100b模型可以调大presence_penalty参数；38b、71b暂时不支持参数调整。

LoRA 的核心只是把参数更新限制在一个低秩子空间上，但损失函数依然是普通的 cross-entropy，所以在小数据 / 大步长的设置下，依旧可能严重偏离原始分布；同时，rank 也很难选：太小学不动复杂模式，太大又增加参数与显存开销、训练不稳、更容易过拟合。
LoraParallelLinearMoE 适配普通张量并行的线性层，低秩矩阵必须与原并行线性层的切分策略对齐 注入 LoRA 配置 + 替换 tp_layer.LoraParallelLinear  强制lora用fp32这是因为 LoRA 更新幅度小，低精度（如 BF16）易下溢
// 根据base_layer类型构建loraABRowParallelLinear → LoRA A 并行，B 全量
ColumnParallelLinear → LoRA A 全量，B 并行
// forward前向关键：先调用 base_layer.forward！
原因：base_layer（如 RowParallelLinear）内部有 AllReduce 或 Gather 操作，不能只拿 .weight 矩阵乘
LoRA 路径：x → dropout → lora_A → lora_B → × scaling → 加到 base 输出

若是 GroupGemmExperts LoraGroupGemmExperts 适配 MoE 专家层。在 MoE 中，每个 token 被路由到少数专家（如 2/8），形成稀疏激活。为高效计算，使用 Grouped GEMM（GMM）： 所有 token 按专家分组 对每组独立做矩阵乘（专家权重 × 该组 token） 最后 unpermute 回原序。如果启用了 专家并行（Expert Parallelism），则 LoRA 参数 不参与全局 AllReduce（因为每个设备的专家是独立的）。
// 将 所有本地专家 的权重 拼接成一个大矩阵：
lora_A: 形状 = (num_exp * d_in, r)
lora_B: 形状 = (num_exp * r, d_out * 2) （*2 因 GLU 激活） 为了高效实现 SwiGLU，大多数 MoE 框架（包括 Megatron）会将 W_up 和 W_gate 的权重拼接在一起，形成一个大的权重矩阵：
这样可在 Grouped GEMM 中一次性计算所有专家的 LoRA 分支
// forward时1、上投影-tokenpermutation-如果有token使用npugemm，reshape矩阵lora_A.weight: (num_exp * d_in, r) → (num_exp, d_in, r)
lora_B.weight: (num_exp * r, d_out*2) → (num_exp, r, d_out*2)2、下投影-先 apply activation + probs mask
再 npu_gmm 计算 base + LoRA3、unpermute4、处理shareed expert

RMSnorm:只做均方根归一化，减少计算

DSA主要解决Q只和K个（KV）计算，不用每个都计算。有个lightening——indexer和topk-select
### MUON
“Moonlight版”，它的缩放因子是通过对齐Adam的Update RMS得到的，从实验中我们观察到，大致上在Warmup结束、模型进入正式训练后，Adam的Update RMS几乎都保持在0.2～0.3之间，并且不同尺寸的模型也呈现出相似的规律。这些模型的共同点是都用Adam训练，参数是β1=0.9,β2=0.95

Muon并不是只管矩阵参数，Muon是只管“稠密输入的线性层的矩阵参数”,那么只需要记住Embedding层和最后的分类层（包括GPT的LM Head）的矩阵参数都不能用Muon，否则效果会明显差。这些不能用Muon的矩阵参数，还有1维、3维及更高维的参数，如果读者不想费太多心思，那么直接用Adam就行，基本上Muon实现都是混合了Adam的，用户可选某些层用Adam。

muon收敛更快，adam优化器梯度的更新主要集中在少数几个方向。muon对矩阵做归一化，计算半正交矩阵在某种程度上放大了在更新中幅度较小但对学习仍然重要的“稀有方向”尺度
Muon 优化器通过矩阵分解，优化了梯度更新方向  过高的Logit数值可能会带来梯度尖刺，严重时会影响训练收敛  MuonClip优化器，在原Muon优化器基础上增加了QK-clip特性，明显降低了损失尖刺出现频率。
RMS系数:梯度平方的指数移动平均（EMA）的平方根，是一个自适应的、基于历史梯度尺度的归一化因子   优化器中，通常使用指数移动平均（EMA）代替简单平均，以更关注近期梯度![img_4.png](img_4.png) 作用是![img_5.png](img_5.png) ADAM
Megatron-LM 默认使用 fused QKV（将 W_Q, W_K, W_V 拼成一个大矩阵 W_QKV）以减少 kernel launch 和 memory access；
但 MuON 实验发现：分别正交化 Q/K/V 效果更好
coefficient_type：选择 牛顿-舒尔茨迭代的多项式系数，决定收敛速度与稳定性。
mode："duplicated"1. All-gather 完整矩阵<br>2. 单 rank 执行 NS 迭代<br>3. Scatter 回各 rank。。通信开销高（All-gather + Scatter）

牛顿-舒尔兹迭代提供了一种免 Hessian、仅用一阶梯度即可逼近逆 Hessian（二阶梯度） 的方法。NS 迭代作用于梯度向量，通过几次多项式迭代，得到一个高精度、噪声鲁棒的更新方向，再结合参数范数构造自适应步长，最终形成一种高效、稳定、二阶感知的一阶优化器。

适配过程：基于Kimi PR 引入的 muon 优化器，并兼容 Distributed Optimizer。若直接用 Patch(DistributedOptimizer, NewClass)，会与已有的 SwapDistributedOptimizer冲突。
解决方案：继承而非替换 → 新建 MuonDistributedOptimizer 继承 DistributedOptimizer，并在 optimizer.py 中注册为新优化器类型。
2、_get_main_param_and_optimizer_states 与 _get_param_groups() 报错
muon 优化器的状态结构与 AdamW 不同（例如没有 exp_avg, exp_avg_sq，而是有 momentum 或其他状态）。
_get_param_groups() 默认假设优化器状态包含 exp_avg 等字段，在 muon 下会 KeyError。
重写 _get_main_param_and_optimizer_states，使其能处理 muon 的 state dict 结构。
在 _get_param_groups() 中增加对 muon 的判断，避免访问不存在的状态字段。

a） kimi pr默认动量系数是0.95, megatron改成了0.9
b）kimi pr 没有 split_QKV,  开启split_QKV能够增加loss收敛速度，不过会增大一点峰值显存和训练时间
kimi pr NS迭代的默认系数（simple）与megatron不同（quintic）

对齐RMS的系数对SFT的影响很大
将对齐rms的系数改为1.0，动量系数改为0.9, NS迭代改为 qutinc
### MOE
计算和通信重叠，在alltoall的时候，分成两个minibatch，前向dispatch的时候后向计算2、在一个batch的时候，后向传梯度和计算W偏导并行
共享专家：缓解专家坍塌，提升通用能力，增强泛化性，qwen3没有共享专家
MOE的辅助loss：1、路由概率熵损失：让router输出更均匀的专家分配概率![img_6.png](img_6.png) 仅约束 router 输出
2、负载均衡损失 同时约束 router 和 专家使用 核心思想：
同时考虑 router 的输出概率 和 实际被选中的专家（top-k）。
惩罚“高概率但低使用率”的专家。解决单条样本的负载不均衡问题
路由器说：“我给专家 e 很高的概率” → 说明它认为 e 适合处理这类 token
但实际：e 却很少被选中（可能因为其他 token 抢占，或 top-k 限制）
→ 这说明 概率分配与实际使用脱节，需要惩罚
![img_7.png](img_7.png)  加入偏执项b
global-batch load balancing loss  确保所有专家被均匀激活

MOE的EP切分：EP 和 TP 是 正交的，即 TP 切分是在 每个专家内部 进行的。All-to-All 通信（核心！） 目的：将 tokens 重分布到持有对应 expert 的 GPU 上
操作： 收集所有 DP+TP 组内的 tokens（实际是 token 表示向量）  根据 expert ID，通过 All-to-All 将 tokens 发送给 EP 组中对应的 GPU!  通信开销分析 [img_8.png](img_8.png)

MTP预测：训练看的远，加入MTP损失，推理的话就是投机算法

正常增训过程中，可以使用alodra构建instruct, 并通过BON采样，偏好引导等方式获取高质量的答案

训练时显存占用大小1、fp16的权重+fp32权重副本2、fp32的优化器状态，分为fp32动量、fp32梯度方差3、fp16的梯度//其他显存消耗每一层的激活值fp16

## 增量预训练路径
Llama3.1技术报告明确指出在预训练最后的阶段提高数学、代码数据可以有效提升模型在关键项上的表现。
一般包含两个部分：
增量预训练：在L0模型的基础上，通过学习行业书籍、论文、标准、操作指导、制度文档等数据，让模型具备一定领域知识。完成增量预训练后的模型只具备文本续写能力丧失了对话问答能力，因此无法应用到具体的行业场景中，因此还需要进行SFT。
SFT：在完成增量预训练后模型的基础上，通过学习行业知识问答、试题、格式文本写作、分类等具体场景的问答数据，让模型具备对话问答能力。SFT流程可一次训练完成，也可根据情况通过多次训练完成。
数据清洗：中文简繁互转，符号标准化，N-gram特征过滤：防止一条文本内部存在重复。文本长度过滤：过长或者过短的文本质量可能较差。全局文本去重：防止多条文本之间存在重复，一般放在最后一步。
数据质检：大模型质检：结合数据实际质量与行业数据编写一段质检提示词，提示词中包含有打分标准与策略，在批量打分完成后，过滤掉不满足要求的数据即可。
增量预训练：按照通用:行业=5:1~10:1配比通用无监督数据。SFT：按照通用:行业=1:1~10:1配比通用有监督数据。

开源金融数据Fin-Eval.领域数据：优先考虑涉及计算等的复杂场景数据，如金融领域的精算、财报分析、投资组合优化等场景。
领域和通用任务配比： 金融领域SFT数据量级比较大，当前领域和通用任务整体维持在1:1的比例，可以在保证通用能力不下降的情况下，整体提升领域任务效果。
不同任务类型的数据配比： self-qa, 网页挖掘问答对，相似检索/标签检索等方式获取的数据， 通常量级非常大，且没有具体的任务指向性（指令明确告诉模型 角色 + 目标 + 约束）。缺乏真实用户意图；无场景约束。实际训练配比过程中，一般可以只训练一个epoch。 而针对具体领域任务针对性构造的数据，则可以适当提升训练epoch数量，通常为3个epoch.
根据训练结果适当调整不同任务数据配比：任务类数据在训练过程中，最好在中间过程中将模型保存并进行自动化评测， 监控每个任务的训练动态， 验证模型在某些任务上是否收敛。 如果某个任务在训练过程中一直成增长趋势， 则可以适当增加该任务训练的epoch数量。 如果某个任务在加入了对应的构造数据之后，反而效果在下降，则大概率数据质量有问题。可以考虑通过偏好引导和BON的方式产生更好的答案。

self-qa:将原始文本数据转换为问答格式，支持（如续写、作文、补全等），适用于构建 QA 数据集。1、清洗文本2、根据内容特征选择续写/补全3、规则生成
单轮对话数据合成：1、选择参看资料，文档片段越长，效果越不好，因此建议对先文档进行切片，使用较短的文档进行问答数据合成，2、题型分流：合成的问答数据需要同时包含多种题型3、把文档片段拼入各类题型模板3、虽然Self-QA是基于给定文本生成问答数据，但仍然无法100%避免幻觉（比如答案未使用文档片段的内容，而是自己编纂），因此需要将模型生成的问答数据（尤其是解释和答案）与给定的文档片段计算相似度，将相似度低于阈值的问答对滤除或者重新生成。
注： 文本相似度算法建议采用ROUGE。 阈值设置没有固定值，需要根据实际情况决定，一般在0.2左右。Reference: “宁德时代2023年海外营收占比23.4%” Candidate: “2023年宁德时代海外收入占比为23.4%” → ROUGE 会认为两者高度相似（因 n-gram 重叠高）


数据管理：基于embedding（gte-large-zh）和 FAISS 向量索引的 SFT 数据去重与查重系统：金融 QA 对语义敏感，阈值不宜过低： 0.95~0.98：严格去重（推荐）

self-instruct（提升问题多样性）:单轮对话数据合成流程：1、选择种子指令&确认指令生成数量2、拼入指令生成提示词模板这里提供了两类不同的提示词模板，可根据实际情况选择： 相似指令生成：生成的指令与提供种子指令的意图、话题、问题类型、风格、难度等更加相似。 指令泛化：基于提供的种子指令进行泛化，生成的指令在保证话题相同的基础上会更加多样化。3、将生成的指令x拼入答案生成提示词模板
通过Self-Instruct生成指令（问题）后，再次调用大模型生成对应的答案。 若生成了多条指令，则需要分别对每条指令依次调用大模型获取每个问题的答案。通常情况下，Self-Instruct或Self-QA合成的指令（问题）都较为简单。在模型SFT时，微调数据中建议包含一定比例的复杂问题（复杂题型、长问题、多个子问题等），以提升模型的泛化能力。同时，考虑到现实世界的大部分问题仍然比较简单，因此也涉及将复杂问题改写简单的场景。
本方法适用于基于给定的指令（问题），通过大模型将其改写为难度更高或者更低的指令（问题）。
注： 给定的指令（old_instruction）只能为1条。 生成指令的数量（num_response_instructions）一般大于1，理论上生成的指令越多质量越差。

拼接种子指令和指令生成数量后，相似指令生成提示词如下所示：
你是一个指令构造专家，你擅长根据要求构造指令。
 
基本要求：
- 如果生成多个指令，需要保证指令的具体任务、措辞以及复杂度的多样性
- 生成的指令相互独立(每个指令都不需要依赖上一个指令的信息， 每个指令都需要包含任务的完整信息， 包含完成该指令所需要的全部背景信息)
- 生成的指令应该能够被生成式语言模型解决，内容完整
 
生成与种子指令相似的指令
指令应该尽量与种子指令的长度保持一致。
 
种子指令： 种子1：什么是社会消费品零售总额？ 种子2：请解释国内生产总值的含义及其计算方式。 种子3：财政赤字是如何产生的？
 
请构造5个相似的指令。

指令质量过滤器，支持自定义打分维度，默认的质量评估打分维度如下:"意图明确性": "判断<指令>的意图是否明确，如果明确，则打1分；否则，打0分"2、"语义清晰性": "判断<指令>的语义是否清晰，如果清晰，则打1分；否则，打0分"3、"指令完整性": "判断<指令>给出的信息是否全面，上下文是否完整, 是否可以回答，如果可回答，则打1分；否则，打0分"
指令质量维度 custom_dimensions = {
    "金融实体明确性": "判断<指令>是否明确提及金融实体（如公司名、产品名、指标名），若是，打1分；否则0分",
    "事实可验证性": "判断<指令>是否涉及可查证的事实（如财报数据、政策条款），若是，打1分；否则0分",
    "任务类型明确性": "判断<指令>是否明确任务类型（如归因、对比、解释、预测），若是，打1分；否则0分"
}

Magpie 生成高质量对齐数据的方法。其核心思想是：利用一个已经对齐的模型，通过“预查询模板”（pre-query templates）来引导其自动生成符合多个维度要求的指令-响应对（instruction-response pairs）
1、生成指令，预查询模板设计，利用对齐 LLM 生成指令2、生成响应，使用同一个对齐 LLM，将上一步生成的指令作为输入，生成对应的模型响应
3、多维标注，质量，难度，任务类别
4、数据过滤 5、数据去重

### 数据清洗
1、pdf清洗，ocr，解析ocr，合并book然后用知识密度得分，通用得分

使用spark清洗电子书txt文件，按照以下规则对txt文本进行清洗：
1、繁体字转简体字
2、特殊字符清洗
3、unicode字符清洗
4、页数相关的数据清洗
5、内容无关的广告、推销等数据清洗
6、注释、参考文献等数据清洗
7、经过\n切分，不是指定标点符号结束的数据清洗

预训练数据分类打分，这种数据集的处理方案是直接使用qwen2-72B打标，然后看分类的分布情况，最后根据分布情况每种类型抽检20条，进行人工确认打标结果是否符合预期，估算准确率。

打标prompt，按照金融细分行业分类 银行 证券 保险 基金 信托 期货 税务 会计
按照金融知识类型分类 金融新闻 金融书籍 金融杂志 金融研报 金融财报 金融招股书 金融公告 金融舆情 金融法律法规

### 数据质量
1、使用效果更好的教师模型或者使用reasoner的summary部分来优化chat模型: 例如，使用R1的content部分作为SFT response, 同样一批instruct，仅仅更换response就可以带来5%以上的效果提升。

2、偏好引导： 如果明确评测标准，可以将客户评判标准写入 System Prompt 来提升模型回复与评测标准一致性。以面试助手为例，评判标准有以下几点：
那么可以将这些要求在数据合成过程中输入给回复生成模型，让回复更贴近评判标准。
面试助手的System Prompt可以这样写：
你是一个优秀的面试官，你的任务是根据招聘岗位和应聘者简历设计面试题。
我们会从以下维度评估你设计的面试问题的质量，在设计过程中请确保面试问题的质量：
1.相关性：生成的面试问题和简历内容（个人信息、教育经历、工作经历）、招聘岗位内容（例如应聘职位、职责信息、职责要求等）的相关程度。相关性越强，面试问题质量越高。
2.遵从性：生成的面试问题必须符合用户要求，包括但不仅限于：人设遵从（例如，站在面试官的角度等），内容遵从（面试问题必须是用户问题中面试者信息完全保持一致）
3.通顺性：语言表达流畅，没有语法语义问题。

3、拒绝采样，同时利用反馈信息，整体提升对齐效果。
有些任务， 即使经过了偏好对齐采样， 依然不能得到有效提升。 主要有两个原因: 1. 即使使用偏好对齐采样，教师模型的答案依然不够好。 2. SFT没有利用负样本，单纯依靠正样本训练，不能让模型充分了解什么样的答案是不合适的。 可以考虑增加反馈信息。

GPT-4o BON采样（qwen-2.5-72b做为裁判模型筛选最优结果）：经过5次采样， 使用qwen-2.5-72b做裁判模型，选择最佳答案。
GPT4o , 不同版本的7b 模型的负样本反馈信息： 使用qwen-2.5-72b对三个模型的输出做BON裁判 （5个 GPT 4o结果，5个7b模型结果：分别来自于不同7b模型 )，输出裁判的思考过程和裁判结论，裁判的思考过程和结论作为SFT的response进行训练。
经过BON采样，同时增加反馈信息，可以使一个比较难优化的任务得到一定的效果提升。

数据过滤: 对于客观题，我们有标准答案。这些数据通过判断对错的方式进行过滤。 对于主观题，没有标准答案，我们使用BON的方式进行过滤：即新爬取的R1结果和原有SFT答案进行对比，如果新爬取的答案质量低于原有SFT答案，则将该答案过滤掉。 另外，规则过滤也可以过滤掉一些明显的问题，例如：中文问题，英文回答。或者summary中含有"<think>"等。

文档内信息重复 处理原则： 通过N-gram方式删除文本中重复出现的内容。

query质检：采用模型质检的方式，从四个维度对指令进行质检： 1、意图明确度：指令表达的目的是否清晰、明确，是否能让接收者（如人或模型）准确理解要完成什么任务。 2、语义清晰度：指令在语言层面上是否具有明确、单一且易于理解的含义。 3、指令完整度：指令给出的信息是否全面，上下文是否完整，是否可以回答。 4、指令复杂度：问题是否复杂，越复杂质量越高。

难度筛选方式：我们一般应用于有标准答案的数据进行难度筛选，通过调用较差模型获取多答案，再经过答案一致性判定答案是正确答案的比例，如果比例超过60%，认定为简单题目，反之为复杂题目。

majority vote方式适用于客观题在无标准答案时，从不同模型回复中筛选出最优的答案。

### 数据配比
A:基础慢思考能力
- 通用慢思考：15w
- 行业慢思考：10w（排除蚂蚁）
- 工行数据：4w（可选）
- 人设/安全/电诈/信用：~0.3w
强调慢思考——即需要多步推理、专业知识、严谨表达的任务；
包含安全合规子集，满足金融监管要求；

B：通用快思考基座方案
- 通用快思考：75w
- 快思考总量：193w（含数学64w、编程64w、逻辑20w、金融4w、通用25w等）
数据以快思考（fast thinking）为主——即事实问答、简单推理、代码生成等；
数学和编程占主导（合计 128w），适合提升 MATH/HumanEval 等基准分数；
金融内容极少（仅 4w），不强调专业性。

C：精选混合训练方案（平衡快慢 + 多轮）
【快思考子采样】
- 数学：20w（从64w中抽）
- 逻辑：20w（从20w中全取或抽）
- 编程：20w（从64w中抽）
- 通用：14w（从75w或25w中选）
- 金融：4w
- 安全：0.2w
- 人设：0.1w

【多轮数据】
- 通用：25w
- 数学20w + 逻辑10w + 编程20w + 通用20w + 金融5w + 安全0.1w + 人设0.1w

对原始快思考数据进行降采样（避免数学/编程过拟合）；
显式加入多轮对话数据，提升上下文理解；
支持人设隔离 + 安全过滤 + 多轮对话的统一模型。

### badcase分析

如何解决裁判模型在“允许多写不扣分”的标准下，仍因“不够简洁”错误扣分的问题。

前情摘要分析
1.  当前case主要是关键点缺失
缺失的关键点类型:   客户身份信息，联系方式，卡号等。 
分析:   关键信息取决于客服对话的摘要目的。 如果只是为了解客服务对话的关键服务内容， 并不需要将客户身份信息和卡号提取出来。 至少这类信息不应该是领域通用的要求。这类问题应该通过在prompt里面指定关键信息的类型来实现，不应该通过模型实现。（训练模型容易破坏已有能力，而且这种定制化偏好不具备任何通用性， 不适合沉淀为L1模型能力）

2.裁判模型不准确
评分标准中明确指出模型如果多回答了关键点不需要扣分， 但是裁判模型对多出来的关键点进行了扣分， 理由是不够简洁

分析：1.  如果参考比较通用的摘要类评测标准（例如FLAME）， 摘要内容应该具有实用性，模型1提到了 "客服准备将客户转接到储蓄卡专员",  这个信息是对后续行动有参考意义的。 但是参考答案并没有列出这一点。
2.  假定"客服准备将客户转接到储蓄卡专员", 不是特别重要， 但是评测标准里面明确指出了多写关键点不会扣分，裁判模型并没有忠实于评测标准，依然进行了扣分，扣分理由是不够简洁。 

改进：在裁判模型的系统指令中显式禁止因简洁性扣分：裁判模型 prompt 强约束： “你只能根据完整性打分。即使回答冗长，只要包含所有关键点且无错误，就得满分。”
改进裁判 prompt（最有效、最常用） 在裁判模型的系统指令中显式禁止因简洁性扣分：

sft后相对L0变差较明显的能力项
基金分析的效果差：现有基金数据可能来自网页爬取，含噪声、过时、无标准答案
解决：1、确保训练数据与目标任务严格对齐 利用已有的 similar-query（相似问题聚类） source（数据来源：研报、公告、客服记录等） tag（任务标签：如“基金费率计算”、“风险等级评估”） 好处：避免用“股票数据”训练“基金模型”，提升数据有效性
2、用参考答案标准重新生成数据：定义“1.64 分回答”的标准，设计强约束 prompt

在ccks数据中， 不能正常生成json结果。
优化方案：——在原始训练数据中注入 1% 的扰动（如空格、换行、转义字符、不间断空格 \xa1 等）——是一种有效的 数据增强策略
真实输入分布：包含各种“无害噪声”（如空格、换行、特殊空白符）
原始训练分布：完全干净
注入扰动后：训练数据 ≈ 真实输入分布


## 知识注入路径

若无监督数据的数据量达不到预训练要求，算力无法支持增量预训练，也无法接受冗长复杂的训练过程，那么增量预训练可能并非最佳选择。
知识注入路径的第一个阶段“知识注入SFT”，需要采用一些规则手段，将无监督数据转换为有监督数据后，再通过SFT的方式学习行业知识。
因此，将无监督数据转换为有监督数据是施行知识注入路径的充要条件，数据转换环节是知识注入路径的最核心流程。以下是一些无监督转有监督的常见任务类型： 文本生成：根据无监督文档的标题、关键词或者简介生成完整文档。//续写//扩写//完型填空//问答//注：
在将无监督数据转换为有监督数据时，需要保证数据的多样性。如：不要将所有文档都仅构建为单一的续写场景，一个比较好的经验是，将不同文档构建为不同的场景，甚至将可以将同一段文本构建为多个不同的场景。
行业有监督数据（知识注入路径阶段1“知识注入SFT”）：推荐50万以上，最少不低于10万条。
行业有监督数据（知识注入路径阶段2“SFT”）：推荐5万条以上，最少不低于1万条。

### 领域微调分享
在人设隔离方案中，“使用通用meta prompt的通用数据”、“使用通用meta prompt的领域数据”以及“使用领域meta prompt的领域数据”三者需要按照一定的比例混合。1、数据回放（混合训练，配比1:1） 2、人设隔离（System Prompt 强引导） 3、 平行数据（为同一任务构造“通用表述”与“领域表述”的成对样本，引导模型学会领域风格迁移，通用meta prompt的领域数据）
总体方案：
领域持续训练阶段混合高质量通用任务指令数据，通过数据回放的持续学习方案，防止模型遗忘通用任务能力
训练阶段通过不同自然语言指令前缀（meta prompt），区分不同领域的指令数据和通用任务数据，确保不同领域数据能得到充分的训练，且不互相干扰
通过构造平行数据的方式，在通用任务的自然语言指令前缀（meta prompt），集成整合各领域能力和通用任务能力，实现模型部署阶段，使用统一meta prompt，处理通用任务和各领域任务，避免针对领域、任务动态选择不同的meta pr
数据混合：
50% 通用指令（通用 meta prompt）
30% 领域指令（领域 meta prompt）
20% 平行数据（通用 meta prompt + 领域内容）

SFT慢思考微调路径

“SFT慢思考微调路径”和“SFT快思考微调路径”虽然都是只做SFT，但由于慢思考微调路径引入了思考过程，在应对数学、逻辑推理等难度与复杂度较高的场景上有更大的优势。因此，若微调场景的难度更高，推荐使用本方案。

数据合成与标注：当前仅有行业无监督数据、只有问题没有答案、问题和答案的质量或数量不满足要求等情况下，需要通过教师模型合成、人工标注等方式获取数据。

数据合成与标注：与快思考有监督数据的几种数据合成方案基本一致，差别是：需要修改数据合成的提示词，使得在生成问题和答案时同时生成思考过程，或者不修改数据合成提示词先获取问题和答案，再基于问题和答案让教师模型反推思考过程。

仅具备完整快思考问答数据的情况下（快思考升级慢思考场景，适用于将历史已具备的快思考有监督数据集更新为快思考有监督数据集）：cot合成

仅具备无监督数据的情况下（先合成问题、答案，再合成思考过程，推荐方案）：

### 多轮对话数据合成
基于已有单轮对话数据，通过大模型分别进行问题生成和答案生成，不断相互迭代得出多轮对话数据。
若当前需要生成的是问题，则将生成问题的完整提示词以单轮问答形式调用大模型A，得到当前轮的问题；若当前需要生成的是答案，则将对话历史以多轮问答形式调用大模型B，得到当前轮问题的答案。
注：
大模型A的作用是生成问题，大模型B的作用是生成答案，因此大模型A和B本质上可以是一个模型，通过不同的调用方式或提示词进行区分即可。
若考虑到效果和效率，则也可以采用不同的模型。生成问题的任务比较简单，因此大模型A可以是一个参数规模较小的模型，注重效率；生成答案的任务更加考验模型能力（或者需要同时生成答案和思考过程），因此大模型B建议是一个能力比较强（或者具有reasoning能力的模型）的模型，注重质量

### 金融项目
分为金融基础能力【金融计算、金融摘要生成等】和金融应用场景【企业风险评估、智能工单生成】
关键目标：企业风险智能评估、欺诈风险识别场景 优于qwen3 32b和DS r1。
工行企业风险智能评估例子：【角色】【任务指令】【风险分析框架】一、信用健康度
- 正向信号：贷记卡额度使用率低且负债很少：金融资产余额增幅高
- 风险信号：贷记卡额度使用率高且负债>行业均值；金融资产余额增幅低

二、经营稳定性
- 正向信号：经营性收入增幅高；四色标识维持灰色
- 风险信号：收入连续降幅高；属于整改类或四色标识为橙/红

三、债务和流动性
- 正向信号：贷款余额/收入比低；未结清机构很少：
- 风险信号：贷款/余额收入比高；未结清机构多；
- 【企业特征数据报告】【强制约束】【输出要求】

模型幻觉成因：1、能力错位：模型能力边界 vs 人类期望功能，训练目标与真实能力脱节，标注数据要求超出能力。如何缓解：明确知识边界声明、引入拒答机制



## QWEN3-VL
把vit编码后的信息加入LLM，8,16,24层的特征。deepstack分别有一个高分辨率和一个低分辨率的图片。
用的是mrope旋转位置编码，3D-rope,6维向量，三组旋转

off-policy，student学习的是teacher频繁出现的上下文
qwen3的训练：1、冷启动训练学习长思维链能力（query筛选，res筛选）2、rl推理能力训练3、思考模式融合4、通用RL
qwen3-next采用线性attention，和标准attention比例3:1，沿用输出门控机制，缓解注意力的低秩问题，采用partial rope（在QK上）QK都要先过一层zero-center rmsnorm，提高长度外推效果。主要是简化attention计算，把O的计算先去掉softmax的归一化，过一个rmsnorm，是等价于softmax 的。把EXP去掉，进行相乘交换律，计算量就变成了O（n)  2、DeltaNet，GDN进一步把遗忘门引入DeltaNet，线性注意力损失函数是欧氏距离

SwiGLU:门控机制是一个sigmoid函数用来控制信息能够通过多少。门控激活函数Swish / SiLU  SiLU其实就是beta为1时的Swish激活函数
![img_9.png](img_9.png)
## 强化学习
policy based:基于polic gradient，不是decent，是incent。最大化reward。theta+lr*gradient*reward

更新policymodel的时候最大的问题是high variance，通过引入baseline解决(稳定rewardsignal)，又提出actor critic（policy，value结合），有一个player（policymodel)和coach(valuemodel)，一起学习

advantage(具体action比平均水平有多好，计算由valuemodel完成) actor critic

trpo-ppo-grpo  

trpo：ratio（新旧策略差别有多大）*advantage，

grpo把valuemodel去掉，暴力baseline的产生，计算advantage，产生多个cot，然后求平均
![img_2.png](img_2.png)


value based



![img.png](img.png)
v=奖励+状态转移概率*Q
Q：选择这个action后一直到最终状态奖励总和的期望   V：在这个状态下一直到最终状态的奖励总和的期望
state状态：当前上下文。action动作：生成下一个token。奖励：生成一个token后能获得的奖励。奖励延迟：需要生成完整一句话后才能赋予奖励（额外训练一个奖励模型）


![img_1.png](img_1.png)
### grpo


## 遇到的问题



## 数据packing



## 训练技巧手册
激活值重计算，flashattention原生支持，激活值和批大小呈线性关系

梯度累计多个微批次计算，和激活重计算一起使用

数据并行是梯度累计的并行版本，使用allreduce，将梯度同步和后向传播重叠，梯度分桶，梯度累计相结合，gpu数量增多会有通信开销瓶颈

所以出现了zero，1.优化器状态分区，对梯度进行reducescatter操作，优化器之后添加全参数的allgather
2、添加对梯度分区进行reducescatter操作3、对模型参数分区，前向和反向都要allgather

zero无法对激活值内存进行处理，所以引入张量并行（缺点：跨节点通信速度较慢，超过8就很明显），所以引入序列并行（主要减少激活值内存，增大批大小和序列长度），随后引入cp，对已经应用张量并行的模块，沿序列长度和另一个维度拆分，在整个模型应用序列拆分，而不是只在序列并行区域，计算梯度之后all-reduce同步cp组内的梯度，attention模块中引入ring-attention，每个gpu先异步将自己的kv发给其他，等待过程中计算已有数据的注意力得分。

zig-zag ringattention解决注意力负载不均的问题，混合排序，每个gpu有早期和晚期的token

pp并行：AFAB到1F1B减少激活内存占用，微批次数量等于或小于pp并行-1时性能较低。还有zerobubble和dualpipe，输入反向B和权重反向W可分离，针对从pp两端传播的两个数据流交错

gpu资源足够，小于10B的模型使用DP就好，10B-30B用DP和TP

## 最近看到的
小米采用混合注意力，具体是 1:5 的全局注意力与滑动窗口注意力混合策略。这里之所以选择混合的 ,是因为小米团队通过实验发现:SWA 简单高效且易于使用,在通用任务、长上下文处理和推理能力上,其整体表现优于线性注意力

采用了一种新的后训练策略:多教师在线策略蒸馏其核心是高效的在线策略学习机制:在通过 SFT/RL 获得领域专家教师模型后,学生模型从自身策略分布中采样(rollout),并利用多个教师提供的密集、逐 token 奖励进行优化。

off-policy训练的缺点在于,学生模型学习的上下文是教师常出现的情境,而不是学生自身经常会遇到的情境。这可能导致累积误差:如果学生在早期犯了个教师从未犯过的错误,它会逐渐偏离 训练中观察到的状态。当我们关注学生在长序列上的表现时,这一问题尤为突出。为了避免这种偏
离,学生必须学会从自身的错误中恢复。off-policy distillation的另一个问题是,学生可能学会模仿教师的风格和自信,但不一定掌握其事实准确性。
