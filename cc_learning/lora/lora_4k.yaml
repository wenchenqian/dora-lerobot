model_config:
  llm:
    tokenizer:
      type: 'PretrainedFromHF'
      path: /data/bucket-pangu-green-guiyang/yuantao/tokenizer/deepseek_R1/
      vocab_size: 129280
      padded_size: 129280
      divided_by: 1
      use_fast: True
    embedding:
      hidden_size: 7168
      init_std: 0.5
      norm:
    position:
      type: 'rope'
      max_size: 163840
      rope_base: 10000.0
      expand_type:
      rope_scaling_type: yarn
      rope_scaling_factor: 40
      rope_scaling_mscale_all_dim: 1.0
      rope_scaling_original_max_position_embeddings: 4096
    blocks:
      num_layers: 61
      init_std: 0.02
      sandwich_norm: False
      layernorm:
        norm_type: 'RMSNorm'
        epsilon: 1e-6
      attention:
        attention_type: "MLA"  #'GQA'
        mla_mm_split:
        q_lora_rank: 1536 #640 #640
        kv_lora_rank: 512 #512 # for rmsnorm in tp=8
        qk_nope_head_dim: 128
        qk_rope_head_dim: 64
        v_head_dim: 128
        head_num: 128
        kv_group_size: 1
        kv_channels: 60
        qkv_bias: False
        projection_bias: False
        dropout_rate: 0.0
        post_norm_scale: 1.0
        mla_fa_without_pad: False
        padded_base_length: 128
        qk_layernorm: True
      ffn:
        hidden_size: 18432
        activation: 'swiglu'
        dropout_rate: 0.0
        mlp_bias: False
        post_norm_scale: 1.0
        pre_norm_scale: 1.0
      moe:
        layers_replaced_as_dense: [0, 1, 2]
        aux_level: seq
        routed_expert_hidden_size: 2048
        post_norm_scale:
        sigmoid_gating: True
        num_experts: 256
        load_balancing_type: noaux_tc
        topk: 8
        shared_expert_hidden_size: 2048
        aux_loss_coeff: 0.0001
        router_type: 'learnable'
        init_std: 0.00103
        no_output_layer_scaled_init: True
        routed_scaling_factor: 2.5
        router_gating_in_fp32: True
        norm_topk_prob: True
        topk_group: 4
        router_bias_update_rate: 0.001
        router_enable_expert_bias: True
        n_shared_experts: 1
        n_group: 8
    head:
      shared_embedding: False


  lora:
    lora_alpha: 32
    lora_load:
    lora_modules_to_save:
    lora_r: 16
    lora_register_forward_hook: word_embeddings input_layernorm
    lora_target_modules: linear_qb linear_kvb linear_proj experts mlp.linear_fc1 mlp.linear_fc2 output_layer
