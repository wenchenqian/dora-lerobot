# Qwen3-VL-32B

**Qwen3VLForConditionalGeneration**

```
Qwen3VLForConditionalGeneration(
  (model): Qwen3VLModel(
    (visual): Qwen3VLVisionModel         # 视觉编码器
    (language_model): Qwen3VLTextModel   # 语言解码器
  )
  (lm_head): Linear(...)                # 语言模型头，用于生成词表概率
)
```

## Qwen3VLVisionModel视觉编码器

### Patch Embedding（`Qwen3VLVisionPatchEmbed`）

`(proj): Conv3d(3, 1152, kernel_size=(2, 16, 16), stride=(2, 16, 16))`

* **使用 ****3D 卷积** 提取图像（或视频）patch。
* `kernel_size=(2, 16, 16)` 暗示模型可处理 **2帧（时间维度）× 16×16 空间 patch****，适合****视频输入****。**
* **输出维度：1152（视觉 token 的 hidden size）。**

![](https://pic4.zhimg.com/v2-ac50b6222eab18e66df2b0984f2e1a0b_1440w.jpg)


**那么3D conv要做的事情就是（相当于拿out\_channels（= vit\_hidden\_size）个3D kernel，每个kernel都在所有patch方块上滚一遍， 得到hidden\_size的一个数值，所有out\_channels个kernel滚完就得到了完整的hidden\_size）：**

![](https://pic3.zhimg.com/v2-64b49e351a9db01e9f209d4d63172710_1440w.jpg)


### Position Embedding

(pos_embed): Embedding(2300+, 1152)

* **为每个视觉 token 添加位置编码。**
* **数量（2304）约为 **`(H/16) × (W/16) × T/2`，例如 224×224 图像 + 2 帧 → `(14×14×2) = 392`，但此处较大，可能支持更高分辨率（如 448×448）。

### 3. Rotary Position Embedding（RoPE）


<pre class="qwen-markdown-code"><div class="qwen-markdown-code-header"><br class="Apple-interchange-newline"/></div></pre>


为多模态输入（文本+图像+视频）生成正确的旋转位置编码索引。

在视觉-语言任务中，视觉 token 并不是一维序列，而是二维（甚至三维）网格（t×h×w），
因此需要一个特殊的 **multi-dimensional RoPE (mRoPE)** 索引。

主要步骤：

根据 input\_ids 中 <vision\_start>、<image\_token>、<video\_token> 的位置判断视觉段落。计算每个视觉区域的三维坐标（t, h, w）。展平后拼接成统一的 position\_ids。 同时生成 mrope\_position\_deltas，用于增量生成阶段（cache位置校正）。这保证了视觉特征在 Transformer 的旋转位置编码中，**空间关系（上下左右）仍然被保留**。在上文中我们提到，Qwen3vl**采用MRoPE-Interleave**，**原始MRoPE将特征维度按照时间（t）、高度（h)和宽度（w)的顺序分块划分，使得时间信息全部分布在高频维度上。在 Qwen3-VL 中采取了 t,h,w 交错分布的形式. 具体实现的细节其实就在get\_rope\_index()这个方法里。**

### 4. Vision Transformer Blocks（27 层）

<pre class="qwen-markdown-code"><div class="qwen-markdown-code-header"><br class="Apple-interchange-newline"/></div></pre>

(blocks): 27 x Qwen3VLVisionBlock

- LayerNorm → Attention → LayerNorm → MLP
- Attention: qkv = Linear(1152 → 3456) → split into Q/K/V
- MLP: 1152 → 4304 → 1152, 激活函数为 GELU-Tanh（更平滑）

**标准 ViT 架构，但使用 ****GELU-Tanh****（一种 GELU 的变体），可能提升训练稳定性**



### Patch Merger（关键创新！）


![](https://pica.zhimg.com/v2-cb870d6c18880ebcce8d5318b4015bec_1440w.jpg)


![](https://pic4.zhimg.com/v2-7970f40304aee502584adc49d786b89b_1440w.jpg)

![](https://pic1.zhimg.com/v2-b5eb25928544af0301ce5bb529f7b4f2_1440w.jpg)

主merger


(merger): Qwen3VLVisionPatchMerger

- 输入: 4608 （= 1152 × 4，推测是 2×2 区域合并）
- 线性变换: 4608 → 4608 → 5120
- 输出维度 5120，与语言模型 hidden size 对齐

![](https://pic2.zhimg.com/v2-1f708b3a85f0cd60557f2e7477270cd5_1440w.jpg)

#### DeepStack Merger（3 个）：

**DeepStack** 的核心思想，将以往多模态大模型（LMM）单层输入视觉tokens的范式，改为在大型语言模型 (LLM) 的多层中进行注入。这种**多层注入**方式旨在实现更精细化的视觉理解。在此基础上，进一步优化了视觉特征 token 化的策略。具体而言，我们将来自 ViT 不同层的视觉特征进行 token 化，并以此作为视觉输入。这种设计能够有效保留从底层（low-level）到高层（high-level）的丰富视觉信息。

(deepstack_merger_list): 3 x Qwen3VLVisionPatchMerger

- 输入: 4608 （可能是中间层特征拼接）
- 用于**多粒度视觉特征融合**（类似 CLIP 的 early+late fusion）


具体而言，模型从 ViT 的不同层提取特征，经 token 化后作为视觉输入，从而在多个层面上丰富语言模型的视觉感知。

DeepStack 是 Qwen3 的视觉特征增强模块。它在由 `deepstack_visual_indexes` 指定的层中提取中间特征，通过多层融合提升整体视觉理解力。每个 DeepStack 层都配有独立的 PatchMerger，并在融合时采用归一化处理（`use_postshuffle_norm=True`），以便实现更稳定的跨层特征整合。


### 假设模型正在理解一张图片，内容是：

“一个穿红衣服的小女孩在草地上吹泡泡。”

### 传统视觉-语言模型的做法：

以往的多模态大模型（如早期的 BLIP 或 Flamingo）通常 **只在语言模型的第一层或输入端** 注入整张图片提取出的视觉 tokens。

模型一开始接收这张图的整体特征（比如“女孩 + 草地 + 泡泡”）， 但在后续的语言生成层中，这些视觉信息会逐渐被“语言上下文”淹没，**结果就是：模型可能记得有个“小女孩”，但忽略了她“穿红衣服”“在吹泡泡”等细节。**

### DeepStack 的改进：

Qwen3-VL 的 DeepStack 技术改变了这种单层注入方式。
它从视觉编码器（ViT）的多个中间层提取出不同层次的视觉特征，比如：

底层特征：颜色、边缘、纹理（识别“红色衣服”、“草地的绿色”）；

中层特征：物体形状、动作（“小女孩”“吹泡泡”）；

高层语义：场景和关系（“女孩在草地上玩”）。

然后，DeepStack 会在语言模型（LLM）的多个解码层**动态注入这些多层视觉特征**。这意味着：

**在模型描述颜色时，它接收底层视觉特征的强化信号；在理解动作或关系时，它得到中层和高层语义的指导。**


### 举个形象比喻：

可以把传统方法想象成**只看一眼照片再讲故事**；
而 DeepStack 就像**在讲故事的过程中不断看回照片**，从不同角度捕捉细节，让生成结果更加准确、生动。

### 在 DeepStack 的帮助下，Qwen3-VL 在描述图像时能更精确地捕捉细节。例如：

“一个穿红衣服的小女孩在绿色的草地上，对着阳光下的泡泡微笑。”这样的描述细腻程度，是单层视觉注入模型难以达到的。

**DeepStack 融合机制** 是 Qwen3-VL 的关键创新之一。它在文本解码阶段持续注入来自视觉编码器中间层的特征，使语言生成过程始终受到视觉语义的引导，模仿了人类“边看边思考”的认知模式。

```
if deepstack_visual_embeds is not None and layer_idx in range(len(deepstack_visual_embeds)):
    hidden_states = self._deepstack_process(
        hidden_states, visual_pos_masks, deepstack_visual_embeds[layer_idx]
    )
```
